{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumayelislam/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ********* Import Packages ********* # \n",
    "import string, random, glob, re, pickle, random, csv, nltk, emoji, operator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from numpy import array, asarray, zeros\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MultiLabelBinarizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_validation import KFold\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Hyper-parameters configurations ********* # \n",
    "\n",
    "# Fix your seed\n",
    "seed = 66\n",
    "np.random.seed(seed)\n",
    "\n",
    "# List of emotions you are going to use in ascending order\n",
    "emotion_categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
    "num_categories = len(emotion_categories)\n",
    "\n",
    "# Word and Hash-emo embedding dimension\n",
    "dimension = 100\n",
    "\n",
    "# Lexical feature dimension\n",
    "feature_dimension = 29\n",
    "\n",
    "filters = [128, 128, 128, 128]\n",
    "dropout_rates = [0.5, 0.5, 0.5, 0.5]\n",
    "kernel_sizes = [1, 2, 3, 1]\n",
    "hidden = [200, 100, 10]\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "embedding_dir = '../Deep-Learning-Resources/embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 21051  data\n",
      "Counter({'joy': 8240, 'surprise': 3849, 'sadness': 3830, 'fear': 2816, 'anger': 1555, 'disgust': 761})\n",
      "literally haven't seen the sun in a week and it's finally coming out!\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "# ********* Load Data ********* # \n",
    "# Important: Change load_data() function according to the dataset that you have. The following \n",
    "# function processes the Twitter Emotion Corpus (TEC)\n",
    "# Link of paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.3384&rep=rep1&type=pdf\n",
    "# Link of dataset: http://saifmohammad.com/WebPages/SentimentEmotionLabeledData.html\n",
    "\n",
    "# List of tweets\n",
    "texts = []\n",
    "\n",
    "# List of labels\n",
    "labels = []\n",
    "\n",
    "def load_data():\n",
    "    with open('tweets.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split()\n",
    "            labels.append(splitted[len(splitted)-1])\n",
    "            texts.append(' '.join(splitted[1:len(splitted)-2]))\n",
    "    print('Loaded %s  data' % len(labels))\n",
    "\n",
    "print(\"Loading data...\")\n",
    "load_data()\n",
    "print(Counter(labels))\n",
    "\n",
    "# Example\n",
    "print(texts[55])\n",
    "print(labels[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Load Lexicons ********* # \n",
    "bingliu_mpqa = {}\n",
    "nrc_emotion = {}\n",
    "nrc_affect_intensity = {}\n",
    "nrc_hashtag_emotion = {}\n",
    "afinn = {}\n",
    "ratings = {}\n",
    "stopwords = []\n",
    "slangs = {}\n",
    "negated = {}\n",
    "emoticons = []\n",
    "\n",
    "# Vader\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def load_lexicons():    \n",
    "    # Ratings by Warriner et al. (2013)\n",
    "    with open('lexicons/Ratings_Warriner_et_al.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "    for i in range(1, len(rows)):\n",
    "        # Normalize values\n",
    "        valence = (float(rows[i][2]) - 1.0)/(9.0-1.0)\n",
    "        arousal = (float(rows[i][5]) - 1.0)/(9.0-1.0)\n",
    "        dominance = (float(rows[i][8]) - 1.0)/(9.0-1.0)\n",
    "        ratings[rows[i][1]] = {\"Valence\": valence, \"Arousal\": arousal, \"Dominance\": dominance}\n",
    "        \n",
    "    \n",
    "    # NRC Emotion Lexicon (2014)\n",
    "    with open('lexicons/NRC-emotion-lexicon-wordlevel-v0.92.txt', 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if splitted[0] not in nrc_emotion:\n",
    "                nrc_emotion[splitted[0]] = {'anger': float(splitted[1]),\n",
    "                                                    'disgust': float(splitted[3]),\n",
    "                                                    'fear': float(splitted[4]),\n",
    "                                                    'joy': float(splitted[5]),\n",
    "                                                    'sadness': float(splitted[8]),\n",
    "                                                    'surprise': float(splitted[9])}\n",
    "\n",
    "    # NRC Affect Intensity (2018)\n",
    "    with open('lexicons/nrc_affect_intensity.txt', 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if splitted[0] not in nrc_affect_intensity:\n",
    "                nrc_affect_intensity[splitted[0]] = {'anger': float(splitted[1]),\n",
    "                                                    'disgust': float(splitted[3]),\n",
    "                                                    'fear': float(splitted[4]),\n",
    "                                                    'joy': float(splitted[5]),\n",
    "                                                    'sadness': float(splitted[8]),\n",
    "                                                    'surprise': float(splitted[9])}\n",
    "                \n",
    "    # NRC Hashtag Emotion Lexicon (2015)\n",
    "    with open('Lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt', 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            splitted[0] = splitted[0].replace('#','')\n",
    "            if splitted[0] not in nrc_hashtag_emotion:\n",
    "                nrc_hashtag_emotion[splitted[0]] = {'anger': float(splitted[1]),\n",
    "                                                    'disgust': float(splitted[3]),\n",
    "                                                    'fear': float(splitted[4]),\n",
    "                                                    'joy': float(splitted[5]),\n",
    "                                                    'sadness': float(splitted[8]),\n",
    "                                                    'surprise': float(splitted[9])}\n",
    "                \n",
    "                \n",
    "    # BingLiu (2004) and MPQA (2005)\n",
    "    with open('lexicons/BingLiu.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if splitted[0] not in bingliu_mpqa:\n",
    "                bingliu_mpqa[splitted[0]] = splitted[1]\n",
    "    with open('lexicons/mpqa.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if splitted[0] not in bingliu_mpqa:\n",
    "                bingliu_mpqa[splitted[0]] = splitted[1]\n",
    "    \n",
    "    \n",
    "    with open('lexicons/AFINN-en-165.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if splitted[0] not in afinn:\n",
    "                score = float(splitted[1])\n",
    "                normalized_score = (score - (-5)) / (5-(-5))\n",
    "                afinn[splitted[0]] = normalized_score\n",
    "                \n",
    "    \n",
    "    with open('lexicons/stopwords.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip())\n",
    "\n",
    "    with open('lexicons/slangs.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split(',', 1)\n",
    "            slangs[splitted[0]] = splitted[1]\n",
    "            \n",
    "    with open('lexicons/negated_words.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            splitted = line.strip().split(',', 1)\n",
    "            negated[splitted[0]] = splitted[1]\n",
    "            \n",
    "    with open('lexicons/emoticons.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            emoticons.append(line.strip())\n",
    "load_lexicons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Helper Functions ********* # \n",
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI\n",
    "\n",
    "def text_has_emoji(text):\n",
    "    for character in text:\n",
    "        if character in emoji.UNICODE_EMOJI:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_tweets(texts):\n",
    "    cleaned_tweets = []\n",
    "    hash_emos = []\n",
    "\n",
    "    for text in texts:\n",
    "        hash_emo = []\n",
    "        text = re.sub('(!){2,}', ' <!repeat> ', text)\n",
    "        text = re.sub('(\\?){2,}', ' <?repeat> ', text)\n",
    "        \n",
    "        # Tokenize using tweet tokenizer\n",
    "        tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        \n",
    "        # Emojis and emoticons\n",
    "        if text_has_emoji(text):\n",
    "            temp = []\n",
    "            for word in tokens:\n",
    "                if char_is_emoji(word):\n",
    "                    hash_emo.append(UNICODE_EMOJI[word])\n",
    "                elif word in emoticons:\n",
    "                    hash_emo.append(word)\n",
    "                else:\n",
    "                    temp.append(word)\n",
    "            tokens = temp\n",
    "            \n",
    "        # Hashtags\n",
    "        temp = []\n",
    "        for word in tokens:\n",
    "            if '#' in word:\n",
    "                word = word.replace('#','')\n",
    "                hash_emo.append(word)\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        tokens = temp\n",
    "            \n",
    "        # Replace slangs and negated words\n",
    "        temp = []\n",
    "        for word in tokens:\n",
    "            if word in slangs:\n",
    "                temp += slangs[word].split()\n",
    "            elif word in negated:\n",
    "                temp += negated[word].split()\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        tokens = temp\n",
    "\n",
    "        # Replace user names\n",
    "        tokens = ['<user>'  if '@' in word else word for word in tokens]\n",
    "        \n",
    "        #Replace numbers\n",
    "        tokens = ['<number>' if word.isdigit() else word for word in tokens]\n",
    "        \n",
    "        # Remove urls\n",
    "        tokens = ['' if 'http' in word else word for word in tokens]\n",
    "        \n",
    "        # Lemmatize\n",
    "        #tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Remove stop words\n",
    "        tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "        # Remove tokens having length 1\n",
    "        tokens = [word for word in tokens if word != '' and len(word) > 1]\n",
    "        \n",
    "        cleaned_tweets.append(tokens)\n",
    "        hash_emos.append(hash_emo)\n",
    "\n",
    "    return cleaned_tweets, hash_emos\n",
    "\n",
    "\n",
    "# This function returns a n-dimensional feature vector\n",
    "def feature_generation(texts, hashtags):\n",
    "    feature_vectors = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        feats = [0] * feature_dimension\n",
    "        for word in texts[i]:\n",
    "            # Warriner er al.\n",
    "            if word in ratings:\n",
    "                feats[0] += ratings[word]['Valence']\n",
    "                feats[1] += ratings[word]['Arousal']\n",
    "                feats[2] += ratings[word]['Dominance']\n",
    "\n",
    "            # Vader Sentiment\n",
    "            polarity_scores = analyzer.polarity_scores(word)\n",
    "            feats[3] += polarity_scores['pos']\n",
    "            feats[4] += polarity_scores['neg']\n",
    "            feats[5] += polarity_scores['neu']\n",
    "\n",
    "            # NRC Emotion\n",
    "            if word in nrc_emotion:\n",
    "                feats[6] += nrc_emotion[word]['anger']\n",
    "                feats[7] += nrc_emotion[word]['disgust']\n",
    "                feats[8] += nrc_emotion[word]['fear']\n",
    "                feats[9] += nrc_emotion[word]['joy']\n",
    "                feats[10] += nrc_emotion[word]['sadness']\n",
    "                feats[11] += nrc_emotion[word]['surprise']\n",
    "\n",
    "            # NRC Affect Intensity\n",
    "            if word in nrc_affect_intensity:\n",
    "                feats[12] += nrc_affect_intensity[word]['anger']\n",
    "                feats[13] += nrc_affect_intensity[word]['disgust']\n",
    "                feats[14] += nrc_affect_intensity[word]['fear']\n",
    "                feats[15] += nrc_affect_intensity[word]['joy']\n",
    "                feats[16] += nrc_affect_intensity[word]['sadness']\n",
    "                feats[17] += nrc_affect_intensity[word]['surprise']\n",
    "\n",
    "            # AFINN\n",
    "            if word in afinn:\n",
    "                feats[18] += float(afinn[word])\n",
    "\n",
    "            # BingLiu and MPQA\n",
    "            if word in bingliu_mpqa:\n",
    "                if bingliu_mpqa[word] == 'positive':\n",
    "                    feats[19] += 1\n",
    "                else:\n",
    "                    feats[20] += 1\n",
    "\n",
    "\n",
    "        count = len(texts[i])\n",
    "        if count == 0:\n",
    "            count = 1\n",
    "        newArray = np.array(feats)/count\n",
    "        feats = list(newArray)\n",
    "        \n",
    "        # Presence of consecutive exclamation mark or question mark\n",
    "        for word in texts[i]:\n",
    "            if word == '<!REPEAT>':\n",
    "                feats[21] = 1\n",
    "            elif word == '<?REPEAT>':\n",
    "                feats[22] = 1\n",
    "\n",
    "        for word in hashtags[i]:\n",
    "            #NRC Hashtag Emotion\n",
    "            if word in nrc_hashtag_emotion:\n",
    "                feats[23] += nrc_hashtag_emotion[word]['anger']\n",
    "                feats[24] += nrc_hashtag_emotion[word]['disgust']\n",
    "                feats[25] += nrc_hashtag_emotion[word]['fear']\n",
    "                feats[26] += nrc_hashtag_emotion[word]['joy']\n",
    "                feats[27] += nrc_hashtag_emotion[word]['sadness']\n",
    "                feats[28] += nrc_hashtag_emotion[word]['surprise']\n",
    "        \n",
    "        feature_vectors.append(feats)\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max([len(s) for s in lines])\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Model ********* # \n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        self.accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        self.accs.append(acc)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "        \n",
    "def model(max_tweet_length, max_hash_emo_length, vocab_size, vocab_size_hash_emo, tweet_matrix, hash_emo_matrix, dimension, feature_dimension, num_categories, train_embedding=False):\n",
    "    # Channel 1\n",
    "    inputs1 = Input(shape=(max_tweet_length,))\n",
    "    embedding1 = Embedding(vocab_size, dimension, weights=[tweet_matrix], trainable=train_embedding)(inputs1)\n",
    "\n",
    "    conv1 = Conv1D(filters=filters[0], kernel_size=kernel_sizes[0], activation='relu')(embedding1)\n",
    "    drop1 = Dropout(dropout_rates[0])(conv1)\n",
    "    pool1 = GlobalMaxPooling1D()(drop1)\n",
    "\n",
    "    conv2 = Conv1D(filters=filters[1], kernel_size=kernel_sizes[1], activation='relu')(embedding1)\n",
    "    drop2 = Dropout(dropout_rates[1])(conv2)\n",
    "    pool2 = GlobalMaxPooling1D()(drop2)\n",
    "\n",
    "    conv3 = Conv1D(filters=filters[2], kernel_size=kernel_sizes[2], activation='relu')(embedding1)\n",
    "    drop3 = Dropout(dropout_rates[2])(conv3)\n",
    "    pool3 = GlobalMaxPooling1D()(drop3)\n",
    "\n",
    "    # Channel 2\n",
    "    inputs2 = Input(shape=(max_hash_emo_length,))\n",
    "    embedding2 = Embedding(vocab_size_hash_emo, dimension, weights=[hash_emo_matrix], trainable=train_embedding)(inputs2)\n",
    "    conv4 = Conv1D(filters=filters[3], kernel_size=kernel_sizes[3], activation='relu')(embedding2)\n",
    "    drop4 = Dropout(dropout_rates[3])(conv4)\n",
    "    pool4 = GlobalMaxPooling1D()(drop4)\n",
    "\n",
    "    # Lexical features\n",
    "    features = Input(shape=(feature_dimension,))\n",
    "\n",
    "    merged = concatenate([pool1, pool2, pool3, pool4, features])\n",
    "    dense1 = Dense(hidden[0], activation='relu')(merged)\n",
    "    dense2 = Dense(hidden[1], activation='relu')(dense1)\n",
    "    dense3 = Dense(hidden[2], activation='relu')(dense2)\n",
    "    outputs = Dense(num_categories, activation='softmax')(dense3)\n",
    "    \n",
    "    model = Model(inputs=[inputs1, inputs2, features], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data...\n",
      "Cleaning Completed!\n",
      "Generating Features...\n",
      "Feature Generation Completed!\n",
      "Encoding Data...\n",
      "Vocabulary size: 24672\n",
      "Vocabulary size (Hash-Emos): 3533\n",
      "Encoding Completed!\n",
      "Loading word embeddings...\n",
      "Loaded 1193514 word vectors.\n",
      "Generating embedding matrices...\n",
      "Embedding matrices genearation completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning Data...\")\n",
    "cleaned_tweets, hash_emos = clean_tweets(texts)\n",
    "print(\"Cleaning Completed!\")\n",
    "\n",
    "print(\"Generating Features...\")\n",
    "features = feature_generation(cleaned_tweets, hash_emos)\n",
    "print(\"Feature Generation Completed!\")\n",
    "\n",
    "\n",
    "print(\"Encoding Data...\")\n",
    "# For Tweet Matrix\n",
    "tokenizer_tweets = create_tokenizer(cleaned_tweets)\n",
    "max_tweet_length = max_length(cleaned_tweets)\n",
    "vocab_size = len(tokenizer_tweets.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "X = encode_text(tokenizer_tweets, cleaned_tweets, max_tweet_length)\n",
    "\n",
    "# For Hash-Emo Matrix\n",
    "tokenizer_hash_emo = create_tokenizer(hash_emos)\n",
    "max_hash_emo_length = max_length(hash_emos)\n",
    "vocab_size_hash_emo = len(tokenizer_hash_emo.word_index) + 1\n",
    "print('Vocabulary size (Hash-Emos): %d' % vocab_size_hash_emo)\n",
    "encoded_hash_emo = encode_text(tokenizer_hash_emo, hash_emos, max_hash_emo_length)\n",
    "\n",
    "# Labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "Y = lb.transform(labels)\n",
    "print(\"Encoding Completed!\")\n",
    "\n",
    "\n",
    "# Load embedding\n",
    "print(\"Loading word embeddings...\")\n",
    "embeddings_index = dict()\n",
    "f = open(embedding_dir)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# Generate embedding matrices\n",
    "print(\"Generating embedding matrices...\")\n",
    "tweet_matrix = zeros((vocab_size, dimension))\n",
    "for word, i in tokenizer_tweets.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        tweet_matrix[i] = np.array(list(embedding_vector))\n",
    "    else:\n",
    "        tweet_matrix[i] = np.array(list(np.random.uniform(low=-1, high=1, size=(100,))))\n",
    "\n",
    "hash_emo_matrix = zeros((vocab_size_hash_emo, dimension))\n",
    "\n",
    "for word, i in tokenizer_hash_emo.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        hash_emo_matrix[i] = np.array(list(embedding_vector))\n",
    "    else:\n",
    "        hash_emo_matrix[i] = np.array(list(np.random.uniform(low=-1, high=1, size=(100,))))\n",
    "print(\"Embedding matrices genearation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1\n",
      "Epoch 1/5\n",
      "18945/18945 [==============================] - 22s 1ms/step - loss: 1.3358 - acc: 0.4757\n",
      "\n",
      "Testing loss: 1.3120272347503692, acc: 0.4795821463054175\n",
      "\n",
      "Epoch 2/5\n",
      "18945/18945 [==============================] - 20s 1ms/step - loss: 1.0934 - acc: 0.5865\n",
      "\n",
      "Testing loss: 1.1313347520991268, acc: 0.5906932576995516\n",
      "\n",
      "Epoch 3/5\n",
      "18945/18945 [==============================] - 20s 1ms/step - loss: 0.9251 - acc: 0.6583\n",
      "\n",
      "Testing loss: 1.0720152396422167, acc: 0.6020892690389584\n",
      "\n",
      "Epoch 4/5\n",
      "18945/18945 [==============================] - 20s 1ms/step - loss: 0.7700 - acc: 0.7198\n",
      "\n",
      "Testing loss: 1.0975413583961986, acc: 0.5978157647654542\n",
      "\n",
      "Epoch 5/5\n",
      "18945/18945 [==============================] - 20s 1ms/step - loss: 0.6131 - acc: 0.7834\n",
      "\n",
      "Testing loss: 1.1148870996600202, acc: 0.6049382714917291\n",
      "\n",
      "acc: 60.49%\n",
      "Fold# 2\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 21s 1ms/step - loss: 1.3815 - acc: 0.4667\n",
      "\n",
      "Testing loss: 1.318865501625804, acc: 0.46840855123877806\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 21s 1ms/step - loss: 1.1468 - acc: 0.5616\n",
      "\n",
      "Testing loss: 1.1642483921911824, acc: 0.5705463182614705\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 22s 1ms/step - loss: 0.9549 - acc: 0.6501\n",
      "\n",
      "Testing loss: 1.1485669934551392, acc: 0.5719714964087389\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 0.7949 - acc: 0.7153\n",
      "\n",
      "Testing loss: 1.1207367792265432, acc: 0.5833729215868861\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.6483 - acc: 0.7738\n",
      "\n",
      "Testing loss: 1.161896063313065, acc: 0.5748218528023809\n",
      "\n",
      "acc: 57.48%\n",
      "Fold# 3\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.3873 - acc: 0.4530\n",
      "\n",
      "Testing loss: 1.319908881640491, acc: 0.4935866984080815\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.1915 - acc: 0.5254\n",
      "\n",
      "Testing loss: 1.2303967296369285, acc: 0.5320665081861183\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.0268 - acc: 0.6112\n",
      "\n",
      "Testing loss: 1.1495162816058995, acc: 0.5757719715247528\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 0.8688 - acc: 0.6825\n",
      "\n",
      "Testing loss: 1.12694447046221, acc: 0.5748218525050655\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 0.7115 - acc: 0.7444\n",
      "\n",
      "Testing loss: 1.1796700811725898, acc: 0.5885985745953267\n",
      "\n",
      "acc: 58.86%\n",
      "Fold# 4\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.3811 - acc: 0.4521\n",
      "\n",
      "Testing loss: 1.288586605303078, acc: 0.5030878859999061\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.1998 - acc: 0.5237\n",
      "\n",
      "Testing loss: 1.2352953179148767, acc: 0.541567695990311\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.0421 - acc: 0.6054\n",
      "\n",
      "Testing loss: 1.22408441341971, acc: 0.5572446553554218\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.8899 - acc: 0.6753\n",
      "\n",
      "Testing loss: 1.2030772252207413, acc: 0.573871733839325\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 26s 1ms/step - loss: 0.7351 - acc: 0.7386\n",
      "\n",
      "Testing loss: 1.1767397233822567, acc: 0.5838479810259405\n",
      "\n",
      "acc: 58.38%\n",
      "Fold# 5\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.4388 - acc: 0.4385\n",
      "\n",
      "Testing loss: 1.3070249865004115, acc: 0.5263657954979396\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.1649 - acc: 0.5509\n",
      "\n",
      "Testing loss: 1.1775648100075982, acc: 0.5719714962105287\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.0047 - acc: 0.6234\n",
      "\n",
      "Testing loss: 1.109711975609605, acc: 0.5857482183574214\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 0.8508 - acc: 0.6915\n",
      "\n",
      "Testing loss: 1.0763022624398637, acc: 0.5999999997734741\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.6900 - acc: 0.7528\n",
      "\n",
      "Testing loss: 1.1191495217506788, acc: 0.5857482184990002\n",
      "\n",
      "acc: 58.57%\n",
      "Fold# 6\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.3892 - acc: 0.4539\n",
      "\n",
      "Testing loss: 1.2928267127261308, acc: 0.5292161519906866\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.1787 - acc: 0.5403\n",
      "\n",
      "Testing loss: 1.2062779535873485, acc: 0.5543942993157267\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 22s 1ms/step - loss: 1.0171 - acc: 0.6159\n",
      "\n",
      "Testing loss: 1.1408939223391426, acc: 0.5733966746126388\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.8550 - acc: 0.6859\n",
      "\n",
      "Testing loss: 1.0995577182452638, acc: 0.5923990499661824\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.6949 - acc: 0.7573\n",
      "\n",
      "Testing loss: 1.1246519356612072, acc: 0.5762470311620174\n",
      "\n",
      "acc: 57.62%\n",
      "Fold# 7\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 25s 1ms/step - loss: 1.5284 - acc: 0.3672\n",
      "\n",
      "Testing loss: 1.4018778909130505, acc: 0.46033254134117\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.3008 - acc: 0.4962\n",
      "\n",
      "Testing loss: 1.3092211343330329, acc: 0.49358669820987133\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.1296 - acc: 0.5562\n",
      "\n",
      "Testing loss: 1.2458166157548047, acc: 0.5211401425461305\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 25s 1ms/step - loss: 0.9823 - acc: 0.6214\n",
      "\n",
      "Testing loss: 1.2262757335309462, acc: 0.5577197147378445\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.8075 - acc: 0.7044\n",
      "\n",
      "Testing loss: 1.1789927590204814, acc: 0.5814726843686682\n",
      "\n",
      "acc: 58.15%\n",
      "Fold# 8\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 26s 1ms/step - loss: 1.7181 - acc: 0.4055\n",
      "\n",
      "Testing loss: 1.6614373933108006, acc: 0.40712589050981607\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 25s 1ms/step - loss: 1.6221 - acc: 0.4162\n",
      "\n",
      "Testing loss: 1.6022018627995833, acc: 0.40332541545043366\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.5710 - acc: 0.4190\n",
      "\n",
      "Testing loss: 1.5655033000574543, acc: 0.40712589050981607\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.5237 - acc: 0.4254\n",
      "\n",
      "Testing loss: 1.4495756465862029, acc: 0.4280285033364194\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.2687 - acc: 0.5128\n",
      "\n",
      "Testing loss: 1.2406054266841282, acc: 0.5182897862515936\n",
      "\n",
      "acc: 51.83%\n",
      "Fold# 9\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 28s 1ms/step - loss: 1.3326 - acc: 0.4822\n",
      "\n",
      "Testing loss: 1.2493177128517712, acc: 0.5358669836560791\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 1.1081 - acc: 0.5862\n",
      "\n",
      "Testing loss: 1.259457397404306, acc: 0.5349168644806551\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 25s 1ms/step - loss: 0.9552 - acc: 0.6466\n",
      "\n",
      "Testing loss: 1.1441614947806062, acc: 0.5729216152726896\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 23s 1ms/step - loss: 0.7871 - acc: 0.7167\n",
      "\n",
      "Testing loss: 1.1576652337706288, acc: 0.5615201901936474\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 27s 1ms/step - loss: 0.6366 - acc: 0.7766\n",
      "\n",
      "Testing loss: 1.1438747897567205, acc: 0.5914489311447053\n",
      "\n",
      "acc: 59.14%\n",
      "Fold# 10\n",
      "Epoch 1/5\n",
      "18946/18946 [==============================] - 27s 1ms/step - loss: 1.3520 - acc: 0.4705\n",
      "\n",
      "Testing loss: 1.2372675380344345, acc: 0.5686460808450422\n",
      "\n",
      "Epoch 2/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 1.1080 - acc: 0.5777\n",
      "\n",
      "Testing loss: 1.1338549809897598, acc: 0.589548693615014\n",
      "\n",
      "Epoch 3/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.9501 - acc: 0.6466\n",
      "\n",
      "Testing loss: 1.0865415516488626, acc: 0.6057007124191791\n",
      "\n",
      "Epoch 4/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.8104 - acc: 0.7079\n",
      "\n",
      "Testing loss: 1.0556514823804841, acc: 0.6123515442827818\n",
      "\n",
      "Epoch 5/5\n",
      "18946/18946 [==============================] - 24s 1ms/step - loss: 0.6553 - acc: 0.7689\n",
      "\n",
      "Testing loss: 1.110120615709989, acc: 0.6066508312406562\n",
      "\n",
      "acc: 60.67%\n",
      "[0.6049382714917291, 0.5833729215868861, 0.5885985745953267, 0.5838479810259405, 0.5999999997734741, 0.5923990499661824, 0.5814726843686682, 0.5182897862515936, 0.5914489311447053, 0.6123515442827818]\n",
      "0.5856719744487286\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(len(labels), n_folds=10, shuffle=True, random_state=seed)\n",
    "\n",
    "accuracies = []\n",
    "counter = 1\n",
    "for train, test in kf:\n",
    "    print('Fold#', counter)\n",
    "    counter += 1\n",
    "    model_GloVe = model(max_tweet_length, \n",
    "                       max_hash_emo_length, \n",
    "                       vocab_size, \n",
    "                       vocab_size_hash_emo, \n",
    "                       tweet_matrix, \n",
    "                       hash_emo_matrix, \n",
    "                       dimension, \n",
    "                       feature_dimension,\n",
    "                       num_categories, \n",
    "                       True)\n",
    "    testObj = TestCallback(([X[test], encoded_hash_emo[test], features[test]], Y[test]))\n",
    "\n",
    "    #earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n",
    "    model_GloVe.fit([X[train], encoded_hash_emo[train], features[train]],\n",
    "                    array(Y[train]),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[testObj],\n",
    "                    verbose = 1)\n",
    "    scores = model_GloVe.evaluate([X[test], encoded_hash_emo[test], features[test]], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model_GloVe.metrics_names[1], scores[1]*100))\n",
    "    index, value = max(enumerate(testObj.accs), key=operator.itemgetter(1))\n",
    "    accuracies.append(value)\n",
    "\n",
    "print(accuracies)\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_GloVe.save(\"model.h5\")\n",
    "import pickle   \n",
    "pickle.dump((lb, tokenizer_tweets, max_tweet_length, tokenizer_hash_emo, max_hash_emo_length, embeddings_index), open( \"variables.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
