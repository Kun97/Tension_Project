{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Import Packages ********* # \n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.metrics import jaccard_distance\n",
    "import string\n",
    "import psutil\n",
    "try:\n",
    "    from urlparse import urlparse\n",
    "except ImportError:\n",
    "    from urllib.parse import urlparse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Initializations ********* # \n",
    "lmtzr = WordNetLemmatizer()\n",
    "hedge_words = []\n",
    "discourse_markers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Python Wrapper for Stanford CoreNLP ********* # \n",
    "# ********* Class definition implemented from \"https://github.com/Lynten/stanford-corenlp\" with slight modifications ********* # \n",
    "\n",
    "class StanfordCoreNLP:\n",
    "    def __init__(self, path_or_host, port=None, memory='4g', lang='en', timeout=1500, quiet=True,\n",
    "                 logging_level=logging.WARNING, max_retries=5):\n",
    "        self.path_or_host = path_or_host\n",
    "        self.port = port\n",
    "        self.memory = memory\n",
    "        self.lang = lang\n",
    "        self.timeout = timeout\n",
    "        self.quiet = quiet\n",
    "        self.logging_level = logging_level\n",
    "\n",
    "        logging.basicConfig(level=self.logging_level)\n",
    "\n",
    "        # Check args\n",
    "        self._check_args()\n",
    "\n",
    "        if path_or_host.startswith('http'):\n",
    "            self.url = path_or_host + ':' + str(port)\n",
    "            logging.info('Using an existing server {}'.format(self.url))\n",
    "        else:\n",
    "\n",
    "            # Check Java\n",
    "            if not subprocess.call(['java', '-version'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT) == 0:\n",
    "                raise RuntimeError('Java not found.')\n",
    "\n",
    "            # Check if the dir exists\n",
    "            if not os.path.isdir(self.path_or_host):\n",
    "                raise IOError(str(self.path_or_host) + ' is not a directory.')\n",
    "            directory = os.path.normpath(self.path_or_host) + os.sep\n",
    "            self.class_path_dir = directory\n",
    "\n",
    "            # Check if the language specific model file exists\n",
    "            switcher = {\n",
    "                'en': 'stanford-corenlp-[0-9].[0-9].[0-9]-models.jar',\n",
    "                'zh': 'stanford-chinese-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n",
    "                'ar': 'stanford-arabic-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n",
    "                'fr': 'stanford-french-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n",
    "                'de': 'stanford-german-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar',\n",
    "                'es': 'stanford-spanish-corenlp-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-models.jar'\n",
    "            }\n",
    "            jars = {\n",
    "                'en': 'stanford-corenlp-x.x.x-models.jar',\n",
    "                'zh': 'stanford-chinese-corenlp-yyyy-MM-dd-models.jar',\n",
    "                'ar': 'stanford-arabic-corenlp-yyyy-MM-dd-models.jar',\n",
    "                'fr': 'stanford-french-corenlp-yyyy-MM-dd-models.jar',\n",
    "                'de': 'stanford-german-corenlp-yyyy-MM-dd-models.jar',\n",
    "                'es': 'stanford-spanish-corenlp-yyyy-MM-dd-models.jar'\n",
    "            }\n",
    "            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n",
    "                raise IOError(jars.get(\n",
    "                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n",
    "\n",
    "            self.port = 9999\n",
    "\n",
    "            # Start native server\n",
    "            logging.info('Initializing native server...')\n",
    "            cmd = \"java\"\n",
    "            java_args = \"-Xmx{}\".format(self.memory)\n",
    "            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n",
    "            class_path = '\"{}*\"'.format(directory)\n",
    "\n",
    "            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n",
    "\n",
    "            args = ' '.join(args)\n",
    "\n",
    "            logging.info(args)\n",
    "\n",
    "            # Silence\n",
    "            with open(os.devnull, 'w') as null_file:\n",
    "                out_file = None\n",
    "                if self.quiet:\n",
    "                    out_file = null_file\n",
    "\n",
    "                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n",
    "                logging.info('Server shell PID: {}'.format(self.p.pid))\n",
    "\n",
    "            self.url = 'http://localhost:' + str(self.port)\n",
    "\n",
    "        # Wait until server starts\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        host_name = urlparse(self.url).hostname\n",
    "        time.sleep(1)  # OSX, not tested\n",
    "        trial = 1\n",
    "        while sock.connect_ex((host_name, self.port)):\n",
    "            if trial > max_retries:\n",
    "                raise ValueError('Corenlp server is not available')\n",
    "            logging.info('Waiting until the server is available.')\n",
    "            trial += 1\n",
    "            time.sleep(1)\n",
    "        logging.info('The server is available.')\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "    def close(self):\n",
    "        logging.info('Cleanup...')\n",
    "        if hasattr(self, 'p'):\n",
    "            try:\n",
    "                parent = psutil.Process(self.p.pid)\n",
    "            except psutil.NoSuchProcess:\n",
    "                logging.info('No process: {}'.format(self.p.pid))\n",
    "                return\n",
    "\n",
    "            if self.class_path_dir not in ' '.join(parent.cmdline()):\n",
    "                logging.info('Process not in: {}'.format(parent.cmdline()))\n",
    "                return\n",
    "\n",
    "            children = parent.children(recursive=True)\n",
    "            for process in children:\n",
    "                logging.info('Killing pid: {}, cmdline: {}'.format(process.pid, process.cmdline()))\n",
    "                # process.send_signal(signal.SIGTERM)\n",
    "                process.kill()\n",
    "\n",
    "            logging.info('Killing shell pid: {}, cmdline: {}'.format(parent.pid, parent.cmdline()))\n",
    "            # parent.send_signal(signal.SIGTERM)\n",
    "            parent.kill()\n",
    "\n",
    "    def annotate(self, text, properties=None):\n",
    "        if sys.version_info.major >= 3:\n",
    "            text = text.encode('utf-8')\n",
    "\n",
    "        r = requests.post(self.url, params={'properties': str(properties)}, data=text,\n",
    "                          headers={'Connection': 'close'})\n",
    "        return r.text\n",
    "\n",
    "    def tregex(self, sentence, pattern):\n",
    "        tregex_url = self.url + '/tregex'\n",
    "        r_dict = self._request(tregex_url, \"tokenize,ssplit,depparse,parse\", sentence, pattern=pattern)\n",
    "        return r_dict\n",
    "\n",
    "    def tokensregex(self, sentence, pattern):\n",
    "        tokensregex_url = self.url + '/tokensregex'\n",
    "        r_dict = self._request(tokensregex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
    "        return r_dict\n",
    "\n",
    "    def semgrex(self, sentence, pattern):\n",
    "        semgrex_url = self.url + '/semgrex'\n",
    "        r_dict = self._request(semgrex_url, \"tokenize,ssplit,depparse\", sentence, pattern=pattern)\n",
    "        return r_dict\n",
    "\n",
    "    def word_tokenize(self, sentence, span=False):\n",
    "        r_dict = self._request('ssplit,tokenize', sentence)\n",
    "        tokens = [token['originalText'] for s in r_dict['sentences'] for token in s['tokens']]\n",
    "\n",
    "        # Whether return token span\n",
    "        if span:\n",
    "            spans = [(token['characterOffsetBegin'], token['characterOffsetEnd']) for s in r_dict['sentences'] for token\n",
    "                     in s['tokens']]\n",
    "            return tokens, spans\n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "    def pos_tag(self, sentence):\n",
    "        r_dict = self._request(self.url, 'pos', sentence)\n",
    "        words = []\n",
    "        tags = []\n",
    "        for s in r_dict['sentences']:\n",
    "            for token in s['tokens']:\n",
    "                words.append(token['originalText'])\n",
    "                tags.append(token['pos'])\n",
    "        return list(zip(words, tags))\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        r_dict = self._request(self.url, 'ner', sentence)\n",
    "        words = []\n",
    "        ner_tags = []\n",
    "        for s in r_dict['sentences']:\n",
    "            for token in s['tokens']:\n",
    "                words.append(token['originalText'])\n",
    "                ner_tags.append(token['ner'])\n",
    "        return list(zip(words, ner_tags))\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        r_dict = self._request(self.url, 'pos,parse', sentence)\n",
    "        return [s['parse'] for s in r_dict['sentences']][0]\n",
    "\n",
    "    def dependency_parse(self, text):\n",
    "        r_dict = self._request(self.url, 'depparse', text)\n",
    "        ls = []\n",
    "        for s in r_dict['sentences']:\n",
    "            tmp = []\n",
    "            for dep in s['basicDependencies']:\n",
    "                tmp.append((dep['dep'], dep['governorGloss'], dep['dependentGloss']))\n",
    "            ls.append(tmp)\n",
    "        return ls\n",
    "\n",
    "    def coref(self, text):\n",
    "        r_dict = self._request('coref', text)\n",
    "\n",
    "        corefs = []\n",
    "        for k, mentions in r_dict['corefs'].items():\n",
    "            simplified_mentions = []\n",
    "            for m in mentions:\n",
    "                simplified_mentions.append((m['sentNum'], m['startIndex'], m['endIndex'], m['text']))\n",
    "            corefs.append(simplified_mentions)\n",
    "        return corefs\n",
    "\n",
    "    def switch_language(self, language=\"en\"):\n",
    "        self._check_language(language)\n",
    "        self.lang = language\n",
    "\n",
    "    def _request(self, url, annotators=None, data=None, *args, **kwargs):\n",
    "        if sys.version_info.major >= 3:\n",
    "            data = data.encode('utf-8')\n",
    "\n",
    "        properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
    "        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n",
    "        if 'pattern' in kwargs:\n",
    "            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n",
    "\n",
    "        logging.info(params)\n",
    "        r = requests.post(url, params=params, data=data, headers={'Connection': 'close'})\n",
    "        r_dict = json.loads(r.text)\n",
    "\n",
    "        return r_dict\n",
    "\n",
    "    def _check_args(self):\n",
    "        self._check_language(self.lang)\n",
    "        if not re.match('\\dg', self.memory):\n",
    "            raise ValueError('memory=' + self.memory + ' not supported. Use 4g, 6g, 8g and etc. ')\n",
    "\n",
    "    def _check_language(self, lang):\n",
    "        if lang not in ['en', 'zh', 'ar', 'fr', 'de', 'es']:\n",
    "            raise ValueError('lang=' + self.lang + ' not supported. Use English(en), Chinese(zh), Arabic(ar), '\n",
    "                                                   'French(fr), German(de), Spanish(es).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Load Lexicons ********* # \n",
    "\n",
    "def load_lexicons():\n",
    "    with open(\"resources/hedge_words.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if '#' in line:\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                hedge_words.append(line.strip())\n",
    "\n",
    "    with open(\"resources/discourse_markers.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if '#' in line:\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                discourse_markers.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********* Initialize CoreNLP ********* # \n",
    "# Download (if you haven't already) the zip file from this link: https://drive.google.com/open?id=1ROwL9fY1-BJ57O5wkgMk4UAxWKretfTk\n",
    "# Unzip the file in the resources folder\n",
    "path = os.path.abspath('resources/stanford-corenlp-full-2018-02-27/')\n",
    "nlp = StanfordCoreNLP(path)\n",
    "load_lexicons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Disambiguate Hedge Terms ********* # \n",
    "# ********* Returns true if (hedge) token is true hedge term, otherwise, returns false ********* # \n",
    "\n",
    "def IsTrueHedgeTerm(hedge, text):\n",
    "    exclude = set(string.punctuation)\n",
    "    \n",
    "    if hedge == \"assume\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if pair[0] == \"ccomp\" and lmtzr.lemmatize(pair[1], 'v') == hedge:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    elif hedge == \"appear\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if (pair[0] in [\"xcomp\", \"ccomp\"]) and lmtzr.lemmatize(pair[1], 'v') == hedge:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    elif hedge == \"suppose\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if pair[0] == \"xcomp\" and lmtzr.lemmatize(pair[1], 'v') == hedge:\n",
    "                token = pair[2]\n",
    "                for temp in tree:\n",
    "                    if temp[0] == \"mark\" and temp[1] == token and temp[2] == \"to\":\n",
    "                        return False\n",
    "        return True\n",
    "    \n",
    "    elif hedge == \"tend\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if pair[0] == \"xcomp\" and lmtzr.lemmatize(pair[1], 'v') == hedge:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    elif hedge == \"should\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if pair[0] == \"aux\" and pair[2] == hedge:\n",
    "                token = pair[1]\n",
    "                for temp in tree:\n",
    "                    if temp[1] == token and temp[2] == \"have\":\n",
    "                        return False\n",
    "        return True\n",
    "    \n",
    "    elif hedge == \"likely\":\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        for pair in tree:\n",
    "            if pair[2] == hedge:\n",
    "                token = pair[1]\n",
    "                for temp in tree:\n",
    "                    if temp[2] == token and temp[1] != \"ROOT\":\n",
    "                        tag = nlp.pos_tag(temp[1])\n",
    "                        if tag[0][1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "                            return False\n",
    "        return True\n",
    "    \n",
    "    elif hedge == \"rather\":\n",
    "        s = ''.join(ch for ch in text if ch not in exclude)\n",
    "        list_of_words = s.split()\n",
    "        next_word = list_of_words[list_of_words.index(hedge) + 1]\n",
    "        if next_word == 'than':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    elif hedge == \"think\":\n",
    "        words = word_tokenize(text)\n",
    "        for i in range(len(words) - 1):\n",
    "            if words[i] == hedge:\n",
    "                tag = nlp.pos_tag(words[i + 1])\n",
    "                if tag[0][1] == \"IN\":\n",
    "                    return False\n",
    "                    break\n",
    "        return True\n",
    "    \n",
    "    elif hedge in [\"feel\", \"suggest\", \"believe\", \"consider\", \"doubt\", \"guess\", \"presume\", \"hope\"]:\n",
    "        parse_trees = nlp.dependency_parse(text)\n",
    "        tree = parse_trees[0]\n",
    "        isRoot = False\n",
    "        hasNSubj = False\n",
    "        for pair in tree:\n",
    "            if lmtzr.lemmatize(pair[2]) in [hedge] and pair[1] == \"ROOT\":\n",
    "                isRoot = True\n",
    "            elif lmtzr.lemmatize(pair[1]) in [hedge] and pair[0] == \"nsubj\":\n",
    "                token = lmtzr.lemmatize(pair[1])\n",
    "                subject = pair[2]\n",
    "                hasNSubj = True\n",
    "\n",
    "        if isRoot and hasNSubj:\n",
    "            tags = nlp.pos_tag(text)\n",
    "            status1 = False\n",
    "            status2 = False\n",
    "            for tag in tags:\n",
    "                if lmtzr.lemmatize(tag[0]) == token and tag[1] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "                    status1 = True\n",
    "                if subject.lower() in [\"i\", \"we\"]:\n",
    "                    status2 = True\n",
    "            if status1 and status2:\n",
    "                return True\n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ********* Determines if a sentence is hedged sentence or not ********* # \n",
    "# ********* Returns true if sentence is hedged sentence, otherwise, returns false ********* # \n",
    "\n",
    "def IsHedgedSentence(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    text = text.lower()\n",
    "    \n",
    "    if \"n't\" in text:\n",
    "        text = text.replace(\"n't\", \" not\")\n",
    "    elif \"n’t\" in text:\n",
    "        text = text.replace(\"n’t\", \" not\")\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    phrases = []\n",
    "    status = False\n",
    "    \n",
    "    # Determine the n-grams of the given sentnece\n",
    "    for i in range(1,6):\n",
    "        phrases += ngrams(tokenized, i)\n",
    "        \n",
    "    # Determine whether disocurse markers are present in the n-grams\n",
    "    # Use Jaccard distance for measuring similarity\n",
    "    for A in discourse_markers:\n",
    "        for B in phrases:\n",
    "            if (1 - jaccard_distance(set(A.split()), set(list(B)))) >= 0.8:\n",
    "                status = True\n",
    "                break\n",
    "\n",
    "        if status:\n",
    "            break\n",
    "\n",
    "    # Determine whether hedge terms are present in the sentence and find out if they are true hedge terms\n",
    "    if not status:\n",
    "        for hedge in hedge_words:\n",
    "            if hedge in tokenized and IsTrueHedgeTerm(hedge, text):\n",
    "                status = True\n",
    "                break\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(IsHedgedSentence(\"I assume he was involved in it.\"))\n",
    "    print(IsHedgedSentence(\"He will assume the role of a counselor.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
